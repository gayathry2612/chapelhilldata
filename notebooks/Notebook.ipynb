{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "surprising-serial",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "# Credits\n",
    "\n",
    "Citation : Ryan Bakker, Liesbet Hooghe, Seth Jolly, Gary Marks, Jonathan Polk, Jan Rovny, Marco Steenbergen, and Milada Anna Vachudova. 2020. “2019 Chapel Hill Expert Survey.” Version 2019.1. Available on chesdata.eu. Chapel Hill, NC: University of North Carolina, Chapel Hill. \n",
    "\n",
    "\n",
    "The 2019_CHES_dataset_means.dta Stata file contains average expert judgments per political\n",
    "party. The 2019_CHES_dataset_expert-level.dta dataset provides information at the level of the\n",
    "individual expert and allows researchers to aggregate expert scores and estimate standard deviations\n",
    "among expert judgments\n",
    "\n",
    "* Completed by : Gayathry Dasika\n",
    "* Purpose : Fun & Education"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "silver-oregon",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import os\n",
    "from pickle import dump\n",
    "from pickle import load\n",
    "\n",
    "# Dimensionality Reduction\n",
    "import umap\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Analysis libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#Visualisation\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "critical-supplier",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jupyter notebook settings\n",
    "pd.set_option('display.max_rows', 10)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "entertaining-kennedy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is session dependent. So creating it here.  \n",
    "try:\n",
    "    os.mkdir('./data/')\n",
    "    os.listdir('.')\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "latter-illustration",
   "metadata": {},
   "source": [
    "# 1: Load data and clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "institutional-elder",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Functions for this section\n",
    "# Test case to compare two lists\n",
    "def checkEqual(L1, L2):\n",
    "    return len(L1) == len(L2) and sorted(L1) == sorted(L2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aware-therapist",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data and load it \n",
    "expert_data = pd.read_stata('https://www.chesdata.eu/s/CHES2019_experts.dta')\n",
    "average_data = pd.read_stata('https://www.chesdata.eu/s/CHES2019V3.dta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prostate-enterprise",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate missing values\n",
    "missing_data = pd.DataFrame(expert_data.isnull().sum().sort_values(ascending=False) / 3823 * 100)\n",
    "plt.title(\"Actual Data in the columns\")\n",
    "plt.scatter(np.arange(0,len(expert_data.isnull().sum())),expert_data.isnull().sum().sort_values(ascending=False) / 3823 * 100)\n",
    "plt.xlabel(\"Column Number\")\n",
    "plt.ylabel(\"% information\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "innocent-console",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Check for parties \n",
    "parties_expert = expert_data['party_id'].unique()\n",
    "parties_average = average_data['party_id'].unique()\n",
    "\n",
    "# There are 277 parties in the survey as per the website. The parties are identified by id\n",
    "assert len(parties_expert) == 277 \n",
    "assert len(parties_average) == 277"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "magnetic-behavior",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ideally the average and expert party ids should match. \n",
    "try:\n",
    "    assert checkEqual(parties_expert, parties_average)\n",
    "except AssertionError:\n",
    "    print(\"Assertion Error, Check the 'party_id' codes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stone-range",
   "metadata": {},
   "source": [
    "### Minor corrections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "representative-indian",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Column rename \n",
    "expert_data = expert_data.rename(columns={'immigra_salience':'immigrate_salience','position':'eu_position'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "offensive-waterproof",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the 843, 1009 in the expert csv with 844, 1016 (code book reference)\n",
    "expert_data.loc[(expert_data[\"party_id\"]==843),\"party_id\"] = 844\n",
    "expert_data.loc[(expert_data[\"party_id\"]==1009),\"party_id\"] = 1016\n",
    "\n",
    "# Run test\n",
    "checkEqual(expert_data[\"party_id\"].unique(),average_data[\"party_id\"].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "military-producer",
   "metadata": {},
   "source": [
    "### Handling missing values and NaNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prostate-blade",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following dictionaries contain column name and unique values in the sample space\n",
    "# Helps in understanding the structure of data.\n",
    "\n",
    "all_columns = {}\n",
    "word_columns = {}\n",
    "num_columns = {}\n",
    "dont_know = {} #special case of csv files\n",
    "\n",
    "for i in expert_data.columns:\n",
    "    print(i)\n",
    "    u = expert_data[i].unique()\n",
    "    print('\\nType:\\n',set([type(k) for k in u]),'\\nValues:\\n',u)\n",
    "    all_columns[i] = u\n",
    "\n",
    "    if i in ['party_name','cname']:\n",
    "        word_columns[i] = u\n",
    "        print(\" : Sorted in word_column\")\n",
    "    elif '.d' in u:\n",
    "        dont_know[i] = u\n",
    "        print(\" : Sorted in dont_know\")\n",
    "    else:\n",
    "        num_columns[i] = u\n",
    "        print(\" : Sorted in num_column\")\n",
    "    print('=='*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "graphic-alliance",
   "metadata": {},
   "source": [
    "### Missing values \n",
    "\n",
    "for numerical columns can be done with mean imputation. \n",
    "The exceptions to columns are demographics columns = gender, age , etc. Mean imputation doesnt disturb the underlying distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incoming-disclosure",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new df \n",
    "df = expert_data.copy()\n",
    "# Replace the numerical NaN with mean value\n",
    "for c in num_columns.keys():    \n",
    "    if c in ['gender','dob']:\n",
    "        df[c] = df[c].replace(to_replace=np.nan,value=None)\n",
    "    else:\n",
    "        df[c] = df[c].fillna(np.mean(df[c]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "million-graduation",
   "metadata": {},
   "source": [
    "### Demographics information \n",
    "Removed from analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extreme-steel",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demographics data -  we will drop it for this analysis \n",
    "personal_details = df[set(expert_data.columns) - set(average_data.columns)]\n",
    "personal_attr = personal_details.columns\n",
    "personal_details[\"party_id\"] = df[\"party_id\"]\n",
    "personal_details[\"party\"] = df[\"party\"].values\n",
    "personal_details = personal_details.set_index(\"party_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "graduate-chinese",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consider numerical columns only \n",
    "df_num = df.drop(columns = personal_details.columns, axis=1)\n",
    "df_num[\"party_a_econ\"] = personal_details[\"party_a_econ\"].values\n",
    "df_num[\"party_b_econ\"] = personal_details[\"party_b_econ\"].values\n",
    "df_num[\"party_c_econ\"] = personal_details[\"party_c_econ\"].values\n",
    "df_num.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acknowledged-light",
   "metadata": {},
   "source": [
    "## Clean data : Numerical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "impossible-service",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_num.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "through-chaos",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to scale these items and remove some unwanted standard deviation columns - as they are not votes\n",
    "\n",
    "u_sd = df_num[[\"lrecon_sd\",\"eu_position_sd\",\"galtan_sd\"]]\n",
    "df_num = df_num.drop(columns=[\"lrecon_sd\",\"eu_position_sd\",\"galtan_sd\"], axis=1)\n",
    "del u_sd\n",
    "df_num.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "funded-things",
   "metadata": {},
   "source": [
    "## Store data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fluid-guide",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data : Save it locally in serialised format\n",
    "\n",
    "df.to_pickle('./data/CHES_Expert_clean.pkl')\n",
    "df_num.to_pickle('./data/CHES_Cleaned_Numerical_data.pkl')\n",
    "personal_details.to_pickle('./data/CHES_Personal_details.pkl')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "czech-ebony",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "characteristic-desperate",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Viewing the summary stats\n",
    "\n",
    "df_num = df_num.set_index('party_id') # Primary key\n",
    "df_num.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thorough-nursing",
   "metadata": {},
   "source": [
    "## Standard Scaling"
   ]
  },
  {
   "cell_type": "raw",
   "id": "acute-version",
   "metadata": {},
   "source": [
    "# Scale the data \n",
    "A dimensionality reduction technique depends on the scaling of the features. \n",
    "We need the measurements to be on the same measuring scale. \n",
    "Here the questionnaires have different scales 0-7, 1-7 ,0-10 etc.\n",
    "\n",
    "Dimensionality reduction\n",
    "Consider a matrix X with N rows (aka \"expert opinion\") and K columns (aka \"questions\"). \n",
    "For this matrix we begin by constructing a variable space with as many dimensions as there are variables. Each variable represents one coordinate axis. For each variable, the length has been standardized according to a scaling criterion, normally by scaling to unit variance. It can be achieved by using Standard Scaler class from sklearn preprocessing library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fiscal-lawsuit",
   "metadata": {},
   "outputs": [],
   "source": [
    "std = StandardScaler()\n",
    "df_num = df_num.reset_index() # To include the party_id : a combo feature of country and party \n",
    "X = std.fit_transform(df_num)\n",
    "df_scale = pd.DataFrame(index = df_num.index, data=X,columns=df_num.columns)\n",
    "df_scale.index = df_num[\"party_id\"]\n",
    "df_scale.to_pickle(\"Standard_numerical_data.pkl\")\n",
    "\n",
    "# store the pickle file \n",
    "dump(std,open('scaler.pkl','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "effective-italic",
   "metadata": {},
   "source": [
    "# 2. Dimensionality Reduction\n",
    "\n",
    "There are around 53 features. To do a small analysis it is ok to plot the items in 2D and do a feature by feature analysis. The complexity we have in this data are 53 numerical features x 32 countries C 2 - A very large number of graphs that will lead us no where. \n",
    "\n",
    "For this purpose, lets attempt to reduce the dimensionality by creating representative 2D features. \n",
    "\n",
    "Techniques discussed :T-SNE, UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affected-finance",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scale = pd.read_pickle('Standard_numerical_data.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "later-filter",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add identifier columns in the end to the data \n",
    "labels = average_data[[\"country\",\"party\",\"party_id\"]]\n",
    "labels = labels.set_index(\"party_id\")\n",
    "df_scale = df_scale.join(how='inner',other=labels)\n",
    "df_scale.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "other-humidity",
   "metadata": {},
   "source": [
    "# Representing in 2D : t-SNE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "military-stamp",
   "metadata": {},
   "source": [
    "The main purpose of t-SNE is visualization of high-dimensional data. Hence, it works best when the data will be embedded on two or three dimensions.\n",
    "\n",
    "Optimizing the KL divergence can be a little bit tricky sometimes. There are five parameters that control the optimization of t-SNE and therefore possibly the quality of the resulting embedding:\n",
    "\n",
    "* perplexity\n",
    "* early exaggeration factor\n",
    "* learning rate\n",
    "* maximum number of iterations\n",
    "* angle (not used in the exact method)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "veterinary-frame",
   "metadata": {},
   "source": [
    "std = load(open('scaler.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "otherwise-chile",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df_scale.loc[:, :'party_c_econ']\n",
    "tsne = TSNE(n_components=2, random_state=0,n_iter=1000,n_jobs=100,verbose=1,perplexity=30)\n",
    "projections = tsne.fit_transform(features)\n",
    "\n",
    "# Data frame for plotting purpose\n",
    "tsne_df = pd.DataFrame(data=projections,index=df_scale.index,columns=[\"X1\",\"X2\"])\n",
    "tsne_df[\"country\"] = df_scale[\"country\"].values\n",
    "tsne_df[\"party\"] = df_scale[\"party\"].values\n",
    "tsne_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "restricted-breeding",
   "metadata": {},
   "source": [
    "## Perplexity plots : t-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "analyzed-question",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility Functions\n",
    "\n",
    "# Function to run a perplexity plot once with settings \n",
    "def run_once_tsne2d(tsne_df=tsne_df,n_iter=1000,n_jobs=100,perplexity=30):\n",
    "    tsne = TSNE(n_components=2, random_state=0,n_iter=n_iter,n_jobs=n_jobs,verbose=1,perplexity=perplexity)\n",
    "    p1 = tsne.fit_transform(features)\n",
    "    print(\"Perplexity :\",perplexity)\n",
    "    tsne_df[\"X1_P\"+str(perplexity)] = p1[:,0]\n",
    "    tsne_df[\"X2_P\"+str(perplexity)] = p1[:,1]\n",
    "    del tsne , p1\n",
    "    return tsne_df\n",
    "\n",
    "# Function to run multiple permutations in a series of steps. The points will be appended to the df\n",
    "def run_permutations_tsne2d(tsne_df=tsne_df,step=5,low=1,high=100,n_iter=1000,n_jobs=100):\n",
    "    projections = dict()\n",
    "    for i in range(low,high,step):\n",
    "        tsne = TSNE(n_components=2, random_state=0,n_iter=n_iter,n_jobs=n_jobs,verbose=1,perplexity=i)\n",
    "        p1 = tsne.fit_transform(features)\n",
    "        print(\"Perplexity :\",i)\n",
    "        projections[\"Perplexity_\"+str(i)] = p1\n",
    "        del p1, tsne\n",
    "    for i in range(1,100,5):\n",
    "        tsne_df[\"X1_P\"+str(i)] = projections['Perplexity_'+str(i)][:,0]\n",
    "        tsne_df[\"X2_P\"+str(i)] = projections['Perplexity_'+str(i)][:,1]\n",
    "    return tsne_df\n",
    "\n",
    "\n",
    "def plot_tsne_1(pp=30,tsne_df=tsne_df.reset_index()):\n",
    "    # Plots graphs with a perplexity setting\n",
    "    try:\n",
    "        x1 = \"X1_P\"+str(pp)\n",
    "        y1 = \"X2_P\"+str(pp)\n",
    "        fig = px.scatter(tsne_df, \n",
    "                         x=x1, \n",
    "                         y= y1,\n",
    "                         color=\"party\",\n",
    "                         opacity=0.4,\n",
    "                         title=\"T-SNE with Perplexity {}\".format(pp))\n",
    "    except:\n",
    "        x1 = \"X1\"\n",
    "        y1 = \"X2\"\n",
    "        fig = px.scatter(tsne_df, \n",
    "                         x=x1, \n",
    "                         y= y1,\n",
    "                         color=\"party\", \n",
    "                         opacity=0.4,\n",
    "                         title=\"T-SNE with Perplexity {}\".format(pp))\n",
    "    return fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "increasing-template",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select perplexity : 51\n",
    "p_select = 51\n",
    "t_df = run_once_tsne2d(tsne_df=tsne_df,perplexity=p_select)\n",
    "plot_tsne_1(p_select,t_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fitting-playing",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select perplexity : 30\n",
    "p_select = 30\n",
    "t_df = run_once_tsne2d(tsne_df=tsne_df,perplexity=p_select)\n",
    "plot_tsne_1(p_select,t_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prescription-villa",
   "metadata": {},
   "source": [
    "The main issue here is lack of clear separation. Also for downstream tasks, might need inverse transform. \n",
    "Inverse transforms are not supported by this package. Will have to pre-process using PCA and that will disturb the local structure of the data. Also t-SNE is computationally expensive. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "geographic-calgary",
   "metadata": {},
   "source": [
    "# 2: Representing in 2D: UMAP \n",
    "For better visualisation as an improved version of t-SNE, as the clusters are not well defined here. \n",
    "There are many settings in UMAP class. Taken the default params for now. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "formed-update",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scale.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "impaired-portal",
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are many components in UMAP, like t-SNE - Trying the default values here\n",
    "umap_2d = umap.umap_.UMAP(n_components=2,init='random',random_state=0).fit(df_scale.loc[:,:'party_c_econ'])\n",
    "projections_2d = umap_2d.transform(df_scale.loc[:,:'party_c_econ'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seasonal-manitoba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe to hold the 2d points\n",
    "umap_df = pd.DataFrame(data=projections_2d,columns=[\"X1\",\"X2\"],index=df_scale.index)\n",
    "umap_df[\"party\"] = df_scale[\"party\"].values\n",
    "umap_df[\"country\"] = df_scale[\"country\"].values\n",
    "umap_df = umap_df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "maritime-affairs",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_2d = px.scatter(umap_df, \n",
    "                    x=\"X1\", \n",
    "                    y=\"X2\",\n",
    "                    color=\"party\", \n",
    "                    labels={'color': 'party'},\n",
    "                    hover_data=[\"X1\",\"X2\",\"party\",\"party_id\",\"country\"],\n",
    "                    title=\"2D UMAP of CHES2019 Data by Parties\")\n",
    "\n",
    "fig_2d_1 = px.scatter(umap_df, \n",
    "                    x=\"X1\", \n",
    "                    y=\"X2\",\n",
    "                    color=\"country\", \n",
    "                    labels={'color': 'party'},\n",
    "                    hover_data=[\"X1\",\"X2\",\"party\",\"party_id\",\"country\"],\n",
    "                    title=\"2D UMAP of CHES2019 Data - Country shade\")\n",
    "\n",
    "fig_2d.update_traces(marker_size=5)\n",
    "fig_2d_1.update_traces(marker_size=5)\n",
    "\n",
    "#Show time\n",
    "fig_2d.show()\n",
    "fig_2d_1.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "periodic-equipment",
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are many components in UMAP, like t-SNE - Trying epoch setting\n",
    "?umap.umap_.UMAP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "developmental-bachelor",
   "metadata": {},
   "source": [
    "## UMAP Settings : experiment\n",
    "Hoping to see a better clustering with more training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prospective-inside",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trying longer epochs - initially was 1000, now 5k\n",
    "umap_2d_k = umap.umap_.UMAP(n_components=2,\n",
    "                               n_epochs=5000,\n",
    "                               verbose=True,\n",
    "                               init='random',\n",
    "                               random_state=0).fit(df_scale.loc[:,:'party_c_econ'])\n",
    "projections_2d_k = umap_2d_k.transform(df_scale.loc[:,:'party_c_econ'])\n",
    "\n",
    "# Create a dataframe to hold the 2d points\n",
    "umap_df_k = pd.DataFrame(data=projections_2d_k,columns=[\"X1\",\"X2\"],index=df_scale.index)\n",
    "umap_df_k[\"party\"] = df_scale[\"party\"].values\n",
    "umap_df_k[\"country\"] = df_scale[\"country\"].values\n",
    "umap_df_k = umap_df_k.reset_index()\n",
    "\n",
    "fig_2d_k = px.scatter(umap_df_k, \n",
    "                    x=\"X1\", \n",
    "                    y=\"X2\",\n",
    "                    color=\"country\", \n",
    "                    labels={'color': 'party'},\n",
    "                    hover_data=[\"X1\",\"X2\",\"party\",\"party_id\",\"country\"],\n",
    "                    title=\"2D UMAP of CHES2019 Data - Country shade, 1000 epochs\")\n",
    "\n",
    "fig_2d_k.update_traces(marker_size=5)\n",
    "#Show time\n",
    "fig_2d_k.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "modular-raising",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_2d_k = px.scatter(umap_df_k, \n",
    "                    x=\"X1\", \n",
    "                    y=\"X2\",\n",
    "                    color=\"party\", \n",
    "                    labels={'color': 'party'},\n",
    "                    hover_data=[\"X1\",\"X2\",\"party\",\"party_id\",\"country\"],\n",
    "                    title=\"2D UMAP of CHES2019 Data - party shade, 5000 epochs\")\n",
    "\n",
    "fig_2d_k.update_traces(marker_size=5)\n",
    "#Show time\n",
    "fig_2d_k.show()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "moral-guyana",
   "metadata": {},
   "source": [
    "# Observation : Epochs\n",
    "More training is taking the samples to a negative space.  \n",
    "Verified it with default, 500, 1000 and 5000 epochs. The numbers dont matter , the clusters seems slightly ok, \n",
    "hard to tell\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hindu-representation",
   "metadata": {},
   "source": [
    "# 3. Sampling 10 New parties\n",
    "Need to pick up some random 10 parties from this distribution and generate the distribution. This is possible with UMAP Inverse transform. Taking the original default values that dont have negative leaning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sustained-intake",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify extreme corners \n",
    "\n",
    "sample = list()\n",
    "for i in range(0,10):\n",
    "    sample.append([np.random.randint(1,11),np.random.randint(2,8)])\n",
    "print(sample)\n",
    "\n",
    "inv_transformed_points = umap_2d.inverse_transform(np.array(sample))\n",
    "random_10 = pd.DataFrame(data=std.inverse_transform(inv_transformed_points),columns=df_scale.loc[:,:'party_c_econ'].columns)\n",
    "random_10[\"party_id\"] = random_10[\"party_id\"].apply(lambda x: int(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "refined-manhattan",
   "metadata": {},
   "outputs": [],
   "source": [
    "alphabet = map(chr, range(97, 97+len(random_10)))\n",
    "random_10[\"party\"] = [_.upper()+\"(r_10)\" for _ in alphabet]\n",
    "random_10[\"country_code\"] = random_10[\"party_id\"].apply(lambda x: x//100)\n",
    "\n",
    "# Traceback opinions in high dimension\n",
    "for i in random_10.columns:\n",
    "    try:\n",
    "        random_10[i] = random_10[i].apply(lambda x: int(x))\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "twelve-grass",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_10.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "immediate-mission",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_10.to_pickle(\"Generated_parties.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "numerous-holiday",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "offshore-ensemble",
   "metadata": {},
   "source": [
    "# 5: Brownie points \n",
    "\n",
    "The feature values in the high-dimensional data set have finite bounds.(0-10)\n",
    "In the 2D space you created in Step 2, paint the area that contains exactly the valid /\n",
    "plausible transformed values corresponding to the high-dimensional feature value bounds.\n",
    "If this is ill-defined with your chosen dimensionality reduction method, try solving the task\n",
    "switching your dimensionality reduction method to principal component analysis instead.\n",
    "Furthermore, in the README, explain why the task was ill-defined with your method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "equivalent-ultimate",
   "metadata": {},
   "outputs": [],
   "source": [
    "umap_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "upset-footage",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.density_heatmap(umap_df,\n",
    "    x=\"X1\", \n",
    "    y=\"X2\", \n",
    "    labels={'color': 'party'},\n",
    "    hover_data=[\"X1\",\"X2\",\"party\",\"party_id\",\"country\"],\n",
    "    title=\"Heatmap of most plausible 2D transformation\",\n",
    "    marginal_x=\"rug\",\n",
    "    marginal_y=\"histogram\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broke-partner",
   "metadata": {},
   "source": [
    "# Aggregating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "million-houston",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scale_agg = df_scale.groupby(by=df_scale.index).mean()\n",
    "df_scale_agg[\"country\"] = average_data[\"country\"].values\n",
    "df_scale_agg[\"party\"] = average_data[\"party\"].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bored-therapist",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UMAP \n",
    "umap_2d_agg = umap.umap_.UMAP(n_components=2,init='random',random_state=0).fit(df_scale_agg.loc[:,:'party_c_econ'])\n",
    "projections_2d_agg = umap_2d_agg.transform(df_scale_agg.loc[:,:'party_c_econ'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "consecutive-basis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe to hold the 2d points\n",
    "umap_df_agg = pd.DataFrame(data=projections_2d_agg,columns=[\"X1\",\"X2\"],index=df_scale_agg.index)\n",
    "umap_df_agg[\"party\"] = df_scale_agg[\"party\"].values\n",
    "umap_df_agg[\"country\"] = df_scale_agg[\"country\"].values\n",
    "umap_df_agg = umap_df_agg.reset_index()\n",
    "umap_df_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "narrow-daisy",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_2 = px.scatter(\n",
    "    umap_df_agg, \n",
    "    x=\"X1\", \n",
    "    y=\"X2\",\n",
    "    color=\"country\", \n",
    "    labels={'color': 'party'},\n",
    "    hover_data=[\"X1\",\"X2\",\"party\",\"party_id\",\"country\"],\n",
    "    title=\"2D UMAP of CHES2019 Data Aggregated Opinions\")\n",
    "\n",
    "fig_2.update_traces(marker_size=10)\n",
    "\n",
    "#Show time\n",
    "fig_2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "noticed-orientation",
   "metadata": {},
   "source": [
    "### Thanks for reading. Hope it was fun ! "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
